{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe6bbd1-ca36-4321-8f82-9a695376dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in d:\\ana\\envs\\nlp-env\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\ana\\envs\\nlp-env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in d:\\ana\\envs\\nlp-env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\ana\\envs\\nlp-env\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\ana\\envs\\nlp-env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.7.1-cp310-cp310-win_amd64.whl (216.1 MB)\n",
      "   ---------------------------------------- 0.0/216.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/216.1 MB 12.0 MB/s eta 0:00:18\n",
      "    --------------------------------------- 3.9/216.1 MB 11.8 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 6.3/216.1 MB 11.7 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 8.7/216.1 MB 11.7 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 11.3/216.1 MB 11.8 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 13.6/216.1 MB 11.7 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 16.0/216.1 MB 11.7 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 18.4/216.1 MB 11.7 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 20.7/216.1 MB 11.8 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 23.1/216.1 MB 11.8 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 25.7/216.1 MB 11.8 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 28.0/216.1 MB 11.8 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 30.4/216.1 MB 11.8 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 33.0/216.1 MB 11.8 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 35.4/216.1 MB 11.8 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 37.7/216.1 MB 11.8 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 40.4/216.1 MB 11.8 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 42.7/216.1 MB 11.8 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 45.1/216.1 MB 11.8 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 47.4/216.1 MB 11.8 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 50.1/216.1 MB 11.8 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 52.4/216.1 MB 11.8 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 54.0/216.1 MB 11.8 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 56.6/216.1 MB 11.7 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 59.0/216.1 MB 11.7 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 61.3/216.1 MB 11.6 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 63.4/216.1 MB 11.6 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 65.8/216.1 MB 11.6 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 67.9/216.1 MB 11.6 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 70.0/216.1 MB 11.5 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 72.4/216.1 MB 11.5 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 73.7/216.1 MB 11.5 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 76.8/216.1 MB 11.4 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 78.6/216.1 MB 11.4 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 81.0/216.1 MB 11.4 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 83.1/216.1 MB 11.3 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 85.2/216.1 MB 11.3 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 87.0/216.1 MB 11.3 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 88.9/216.1 MB 11.2 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 89.9/216.1 MB 11.2 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 91.2/216.1 MB 10.9 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 91.8/216.1 MB 10.7 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 92.5/216.1 MB 10.6 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 93.6/216.1 MB 10.4 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 94.6/216.1 MB 10.3 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 95.7/216.1 MB 10.2 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 96.7/216.1 MB 10.1 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 98.0/216.1 MB 10.0 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 99.4/216.1 MB 9.9 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 100.4/216.1 MB 9.8 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 101.2/216.1 MB 9.7 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 102.8/216.1 MB 9.6 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 104.3/216.1 MB 9.6 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 105.6/216.1 MB 9.5 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 107.2/216.1 MB 9.5 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 108.8/216.1 MB 9.5 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 110.6/216.1 MB 9.4 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 112.2/216.1 MB 9.4 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 113.8/216.1 MB 9.4 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 115.3/216.1 MB 9.4 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 116.9/216.1 MB 9.4 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 118.5/216.1 MB 9.3 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 120.3/216.1 MB 9.3 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 121.9/216.1 MB 9.3 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 123.7/216.1 MB 9.3 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 125.3/216.1 MB 9.3 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 126.9/216.1 MB 9.2 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 128.2/216.1 MB 9.2 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 129.5/216.1 MB 9.2 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 131.1/216.1 MB 9.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 132.6/216.1 MB 9.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 134.5/216.1 MB 9.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 136.1/216.1 MB 9.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 137.9/216.1 MB 9.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 139.5/216.1 MB 9.1 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 141.3/216.1 MB 9.1 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 142.9/216.1 MB 9.1 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 144.7/216.1 MB 9.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 146.5/216.1 MB 9.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 148.6/216.1 MB 9.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 151.3/216.1 MB 9.1 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 153.1/216.1 MB 9.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 154.7/216.1 MB 9.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 156.8/216.1 MB 9.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 158.6/216.1 MB 9.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 160.4/216.1 MB 9.1 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 162.5/216.1 MB 9.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 164.6/216.1 MB 9.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 166.7/216.1 MB 9.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 168.6/216.1 MB 9.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 170.9/216.1 MB 9.2 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 173.3/216.1 MB 9.2 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 175.6/216.1 MB 9.2 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 178.0/216.1 MB 9.2 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 180.6/216.1 MB 9.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 183.2/216.1 MB 9.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 185.6/216.1 MB 9.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 188.0/216.1 MB 9.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 190.6/216.1 MB 9.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 192.9/216.1 MB 9.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 195.3/216.1 MB 9.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 197.7/216.1 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 200.0/216.1 MB 9.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 202.4/216.1 MB 9.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 205.0/216.1 MB 9.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 207.4/216.1 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 210.0/216.1 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  212.3/216.1 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  214.7/216.1 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  216.0/216.1 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 216.1/216.1 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 2.4/6.3 MB 11.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 11.3 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 11.7 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "\n",
      "   ---------------------------------------- 0/4 [mpmath]\n",
      "   ---------------------------------------- 0/4 [mpmath]\n",
      "   ---------------------------------------- 0/4 [mpmath]\n",
      "   ---------------------------------------- 0/4 [mpmath]\n",
      "   ---------------------------------------- 0/4 [mpmath]\n",
      "   ---------------------------------------- 0/4 [mpmath]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [sympy]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   -------------------- ------------------- 2/4 [networkx]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ------------------------------ --------- 3/4 [torch]\n",
      "   ---------------------------------------- 4/4 [torch]\n",
      "\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ecb105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download(\"all\")\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a856b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'data/train-00000-of-00001-daf190ce720b3dbb.parquet', 'test': 'data/test-00000-of-00001-fa9b3e8ade89a333.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/Deysi/spam-detection-dataset/\" + splits[\"train\"])\n",
    "df_test =  pd.read_parquet(\"hf://datasets/Deysi/spam-detection-dataset/\" + splits[\"test\"])\n",
    "# df_train = pd.read_parquet(\"spam-detection-dataset (2).parquet\")\n",
    "# df_test = pd.read_parquet(\"spam-detection-dataset.parquet\")\n",
    "df = pd.concat([df_train, df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d34085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hey I am looking for Xray baggage datasets can...</td>\n",
       "      <td>not_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Get rich quick! Make millions in just days wi...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>URGENT MESSAGE: YOU WON'T BELIEVE WHAT WE HAVE...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Google AI Blog: Contributing Data to Deepfake...</td>\n",
       "      <td>not_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trying to see if anyone already has timestamps...</td>\n",
       "      <td>not_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10895</th>\n",
       "      <td>Is it good and usable?\\n\\n[https://www.uscompa...</td>\n",
       "      <td>not_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10896</th>\n",
       "      <td>I'm not sure if this is the absolute best sub ...</td>\n",
       "      <td>not_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10897</th>\n",
       "      <td>Would love if anyone knew of any really good d...</td>\n",
       "      <td>not_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10898</th>\n",
       "      <td>Fields = Hashrate, VRAM, TDP, MSRP, Profit/day</td>\n",
       "      <td>not_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10899</th>\n",
       "      <td>Feelin’ like you’re not getting enough attenti...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text     label\n",
       "0      hey I am looking for Xray baggage datasets can...  not_spam\n",
       "1      \"Get rich quick! Make millions in just days wi...      spam\n",
       "2      URGENT MESSAGE: YOU WON'T BELIEVE WHAT WE HAVE...      spam\n",
       "3      [Google AI Blog: Contributing Data to Deepfake...  not_spam\n",
       "4      Trying to see if anyone already has timestamps...  not_spam\n",
       "...                                                  ...       ...\n",
       "10895  Is it good and usable?\\n\\n[https://www.uscompa...  not_spam\n",
       "10896  I'm not sure if this is the absolute best sub ...  not_spam\n",
       "10897  Would love if anyone knew of any really good d...  not_spam\n",
       "10898     Fields = Hashrate, VRAM, TDP, MSRP, Profit/day  not_spam\n",
       "10899  Feelin’ like you’re not getting enough attenti...      spam\n",
       "\n",
       "[10900 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d6b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text'].to_numpy() \n",
    "labels = df['label'].to_numpy()  \n",
    "\n",
    "assert len(texts) == len(labels), \"Arrays must be of equal length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a4790eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize(texts):\n",
    "    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): List of text data\n",
    "\n",
    "    Returns:\n",
    "        tokenized_texts (List[List[str]]): List of list of tokens\n",
    "        word2idx (Dict): Vocabulary built from the corpus\n",
    "        max_len (int): Maximum sentence length\n",
    "    \"\"\"\n",
    "\n",
    "    max_len = 0\n",
    "    tokenized_texts = []\n",
    "    word2idx = {}\n",
    "\n",
    "    # Add <pad> and <unk> tokens to the vocabulary\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "\n",
    "    # Building our vocab from the corpus starting from index 2\n",
    "    idx = 2\n",
    "    for sent in texts:\n",
    "        # Tokenize a sentence\n",
    "        # CODE_START\n",
    "        tokenized_sent = word_tokenize(sent.lower())\n",
    "        # CODE_END\n",
    "\n",
    "        # Add `tokenized_sent` to `tokenized_texts`\n",
    "        tokenized_texts.append(tokenized_sent)\n",
    "\n",
    "        # Add new token to `word2idx`\n",
    "        for token in tokenized_sent:\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "        # Update `max_len`\n",
    "        max_len = max(max_len, len(tokenized_sent))\n",
    "\n",
    "    return tokenized_texts, word2idx, max_len\n",
    "def encode(tokenized_texts, word2idx, max_len):\n",
    "    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n",
    "    their index in the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        input_ids (np.array): Array of token indexes in the vocabulary with\n",
    "            shape (N, max_len). It will be the input to our CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = []\n",
    "    for tokenized_sent in tokenized_texts:\n",
    "        # Pad sentences to max_len\n",
    "        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
    "\n",
    "        # Encode tokens to input_ids\n",
    "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
    "        input_ids.append(input_id)\n",
    "\n",
    "    return np.array(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d52746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "\n",
      "Loading pretrained vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\член\\AppData\\Local\\Temp\\ipykernel_3432\\725594635.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for line in tqdm_notebook(fin):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81dc1b2ea2a043379f3158e64bdbf1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19079 / 31540 pretrained vectors found.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def load_pretrained_vectors(word2idx, fname):\n",
    "    \"\"\"Load pretrained vectors and create embedding layers.\n",
    "\n",
    "    Args:\n",
    "        word2idx (Dict): Vocabulary built from the corpus\n",
    "        fname (str): Path to pretrained vector file\n",
    "\n",
    "    Returns:\n",
    "        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n",
    "            the size of word2idx and d is embedding dimension\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading pretrained vectors...\")\n",
    "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "\n",
    "    # Initialize random embeddings\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
    "    embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
    "\n",
    "    # Load pretrained vectors\n",
    "    count = 0\n",
    "    for line in tqdm_notebook(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        word = tokens[0]\n",
    "        if word in word2idx:\n",
    "            count += 1\n",
    "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
    "\n",
    "    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Tokenize, build vocabulary, encode tokens\n",
    "print(\"Tokenizing...\\n\")\n",
    "tokenized_texts, word2idx, max_len = tokenize(texts)\n",
    "input_ids = encode(tokenized_texts, word2idx, max_len)\n",
    "\n",
    "# Load pretrained vectors\n",
    "embeddings = load_pretrained_vectors(word2idx, \"crawl-300d-2M.vec\")\n",
    "embeddings = torch.tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76fee3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "\n",
    "def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n",
    "                batch_size=10):\n",
    "    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n",
    "    DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    # CODE_START\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    val_inputs = torch.tensor(val_inputs)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    val_labels = torch.tensor(val_labels)\n",
    "    # CODE_END\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    # CODE_START\n",
    "    val_data = TensorDataset(val_inputs, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "    # CODE_END\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0c9aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels_int = le.fit_transform(labels)\n",
    "\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    input_ids,\n",
    "    labels_int,\n",
    "    test_size=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader, val_dataloader = data_loader(\n",
    "    train_inputs,\n",
    "    val_inputs,\n",
    "    train_labels,\n",
    "    val_labels,\n",
    "    batch_size=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ecf713b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: ['not_spam' 'spam']\n",
      "Label types: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels:\", np.unique(labels))\n",
    "print(\"Label types:\", type(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e718faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It seems like I need to change spam/not spam to 0/1, and only after do everything else\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "labels = np.array(encoded_labels, dtype=np.int64)\n",
    "#yooo it was easy to fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4659a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay, now i need move on \n",
    "#rerunning this:\n",
    "def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n",
    "                batch_size=10):\n",
    "    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n",
    "    DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    # CODE_START\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    val_inputs = torch.tensor(val_inputs)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    val_labels = torch.tensor(val_labels)\n",
    "    # CODE_END\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    # CODE_START\n",
    "    val_data = TensorDataset(val_inputs, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "    # CODE_END\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "#then splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CODE_START\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    input_ids, labels, \n",
    "    test_size=0.05, \n",
    "    random_state=42, \n",
    "#     stratify=encoded_labels # !!!\n",
    ")\n",
    "# CODE_END\n",
    "\n",
    "# Convert to proper numeric type\n",
    "labels = np.array(labels, dtype=np.int64)\n",
    "\n",
    "# Load data to PyTorch DataLoader\n",
    "# Use batch_size = 50\n",
    "# CODE_START\n",
    "train_dataloader, val_dataloader = data_loader(\n",
    "    train_inputs, val_inputs,\n",
    "    train_labels, val_labels,\n",
    "    batch_size=50\n",
    ")\n",
    "# CODE_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "667b2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_NLP(nn.Module):\n",
    "    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\"\"\"\n",
    "    def __init__(self,\n",
    "                 pretrained_embedding=None,\n",
    "                 freeze_embedding=False,\n",
    "                 vocab_size=None,\n",
    "                 embed_dim=300,\n",
    "                 filter_sizes=[3, 4, 5],\n",
    "                 num_filters=[100, 100, 100],\n",
    "                 num_classes=2,\n",
    "                 dropout=0.5):\n",
    "        \"\"\"\n",
    "        The constructor for CNN_NLP class.\n",
    "        \"\"\"\n",
    "        super(CNN_NLP, self).__init__()\n",
    "        # Embedding layer\n",
    "        if pretrained_embedding is not None:\n",
    "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
    "                                                          freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                          embedding_dim=self.embed_dim,\n",
    "                                          padding_idx=0,\n",
    "                                          max_norm=5.0)\n",
    "        # Conv Network\n",
    "        # CODE_START\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self.embed_dim,\n",
    "                      out_channels=num_filters[i],\n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))\n",
    "        ])\n",
    "        # CODE_END\n",
    "        \n",
    "        # Fully-connected layer and Dropout\n",
    "        # CODE_START\n",
    "        self.fc = nn.Linear(sum(num_filters), num_classes)\n",
    "        # CODE_END\n",
    "\n",
    "        # Dropout \n",
    "        # CODE_START\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # CODE_END \n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"Perform a forward pass through the network.\"\"\"\n",
    "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
    "        x_embed = self.embedding(input_ids).float()\n",
    "\n",
    "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
    "        # Output shape: (b, embed_dim, max_len)\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
    "        # CODE_START\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "        # CODE_END\n",
    "\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "\n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "\n",
    "        # Compute logits. Output shape: (b, n_classes)\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d886cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def initialize_model(pretrained_embedding=None,\n",
    "                    freeze_embedding=False,\n",
    "                    vocab_size=None,\n",
    "                    embed_dim=300,\n",
    "                    filter_sizes=[3, 4, 5],\n",
    "                    num_filters=[100, 100, 100],\n",
    "                    num_classes=2,\n",
    "                    dropout=0.5,\n",
    "                    learning_rate=0.01):\n",
    "    \"\"\"Instantiate a CNN model and an optimizer.\"\"\"\n",
    "\n",
    "    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and \\\n",
    "    num_filters need to be of the same length.\"\n",
    "\n",
    "    # Instantiate CNN model\n",
    "    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n",
    "                        freeze_embedding=freeze_embedding,\n",
    "                        vocab_size=vocab_size,\n",
    "                        embed_dim=embed_dim,\n",
    "                        filter_sizes=filter_sizes,\n",
    "                        num_filters=num_filters,\n",
    "                        num_classes=num_classes,  \n",
    "                        dropout=dropout)         \n",
    "\n",
    "    # Send model to `device` (GPU/CPU)\n",
    "    cnn_model.to('cpu')\n",
    "\n",
    "    # Instantiate RMSprop optimizer\n",
    "    # CODE_START\n",
    "    optimizer = optim.RMSprop(cnn_model.parameters(), lr=learning_rate)\n",
    "    # CODE_END\n",
    "\n",
    "    return cnn_model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b7d7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n",
    "    \"\"\"Train the CNN model.\"\"\"\n",
    "\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "\n",
    "        # Tracking time and loss\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_labels = tuple(t.to('cpu') for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            # CODE_START\n",
    "            optimizer.zero_grad()\n",
    "            # CODE_END\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            # CODE_START\n",
    "            logits = model(b_input_ids)\n",
    "            # CODE_END\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            # CODE_START\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            total_loss += loss.item()\n",
    "            # CODE_END \n",
    "\n",
    "            # Perform a backward pass to calculate gradients and update parameters\n",
    "            # CODE_START\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # CODE_END\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        # CODE_START\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        # CODE_END\n",
    "\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if val_dataloader is not None:\n",
    "            # After the completion of each training epoch, measure the model's\n",
    "            # performance on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Track the best accuracy\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's\n",
    "    performance on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled\n",
    "    # during the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels = tuple(t.to('cpu') for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f386480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    | 1773.878858  | 44.603037  |   99.27   |  742.90  \n",
      "   2    |  793.424300  | 71.368140  |   99.45   |  775.02  \n",
      "   3    |  853.437465  | 163.853664 |   98.91   |  754.65  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 99.45%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "set_seed(42)\n",
    "cnn_rand, optimizer = initialize_model(vocab_size=len(word2idx),\n",
    "                                      embed_dim=300,\n",
    "                                      learning_rate=0.25,\n",
    "                                      dropout=0.5)\n",
    "train(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c846993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# CNN-static: fastText pretrained word vectors are used and freezed during training.\n",
    "set_seed(42)\n",
    "cnn_static, optimizer = initialize_model(pretrained_embedding=embeddings,\n",
    "                                        freeze_embedding=True,\n",
    "                                        learning_rate=0.25,\n",
    "                                        dropout=0.5)\n",
    "train(cnn_static, optimizer, train_dataloader, val_dataloader, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e4e5d-ae34-4313-8718-1587ad32d07c",
   "metadata": {},
   "source": [
    "# Грузило, і вимкнули світло в будинку)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-non-static: fastText pretrained word vectors are fine-tuned during training.\n",
    "set_seed(42)\n",
    "cnn_non_static, optimizer = initialize_model(pretrained_embedding=embeddings,\n",
    "                                            freeze_embedding=False,\n",
    "                                            learning_rate=0.25,\n",
    "                                            dropout=0.5)\n",
    "train(cnn_non_static, optimizer, train_dataloader, val_dataloader, epochs=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3830b64a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cnn_rand'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcnn_rand\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(text, model\u001b[38;5;241m=\u001b[39mcnn_rand\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m62\u001b[39m):\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict probability that a review is positive.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cnn_rand'"
     ]
    }
   ],
   "source": [
    "import cnn_rand\n",
    "def predict(text, model=cnn_rand.to(\"cpu\"), max_len=62):\n",
    "    \"\"\"Predict probability that a review is positive.\"\"\"\n",
    "\n",
    "    # Tokenize, pad and encode text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n",
    "    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n",
    "\n",
    "    # Compute logits\n",
    "    logits = model.forward(input_id)\n",
    "\n",
    "    #  Compute probability\n",
    "    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n",
    "\n",
    "    print(f\"This entry is {probs[1] * 100:.2f}% not spam.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1272b2dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCongratulations! You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve won a free vacation!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m probability \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m(test_text, model\u001b[38;5;241m=\u001b[39mcnn_rand)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(probability)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "test_text = \"Congratulations! You've won a free vacation!\"\n",
    "probability = predict(test_text, model=cnn_rand)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec1b3ec-93c1-478c-ae49-2b80c498780f",
   "metadata": {},
   "source": [
    "# на жаль немає часу грузити ще раз("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd52e7d-a026-4545-9a6b-efe6ba74201d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
